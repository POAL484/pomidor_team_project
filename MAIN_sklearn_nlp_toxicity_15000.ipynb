{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"datasets/train_15000.csv\")\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    sentence = re.sub(r\"what's\", \"what is \", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" \", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have \", sentence)\n",
    "    sentence = re.sub(r\"can't\", \"can not \", sentence)\n",
    "    sentence = re.sub(r\"n't\", \" not \", sentence)\n",
    "    sentence = re.sub(r\"i'm\", \"i am \", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are \", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would \", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will \", sentence)\n",
    "    sentence = re.sub(r\"\\'scuse\", \" excuse \", sentence)\n",
    "    sentence = re.sub('\\W', ' ', sentence)\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    sentence = re.sub(r'[?|!|\\'|\"|#–—-]',r'',sentence)\n",
    "    sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)\n",
    "    sentence = ' '.join([word for word in sentence.split() if word not in stop_words])\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.replace(\"\\n\",\" \")\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmedSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemmedSentence += stem\n",
    "        stemmedSentence += \" \"\n",
    "    stemmedSentence = stemmedSentence.strip()\n",
    "    return stemmedSentence\n",
    "\n",
    "df['comment_text'] = df['comment_text'].map(lambda x : clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train, test = sklearn.model_selection.train_test_split(df, random_state=42, test_size=0.1, shuffle=True)\n",
    "X_train = train.comment_text\n",
    "X_test = test.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(len(categories)):\n",
    "    models.append(Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsOneClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "            ]))\n",
    "for i in range(len(categories)):\n",
    "    models[i].fit(X_train, train[categories[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.9590800852237122\n",
      "f1_score: 0.7391130643228125\n",
      "accuracy_score: 0.990913648326858\n",
      "f1_score: 0.3438914027149321\n",
      "accuracy_score: 0.9775661110414839\n",
      "f1_score: 0.746458923512748\n",
      "accuracy_score: 0.9977440782052889\n",
      "f1_score: 0.2173913043478261\n",
      "accuracy_score: 0.9703596941972679\n",
      "f1_score: 0.6347490347490348\n",
      "accuracy_score: 0.991978944729916\n",
      "f1_score: 0.2727272727272727\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(categories)):\n",
    "    prediction = models[i].predict(X_test)\n",
    "    print('accuracy_score:', (accuracy_score(test[categories[i]], prediction)))\n",
    "    print('f1_score:', (f1_score(test[categories[i]], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_categories(models, categories, text):\n",
    "    text = clean_text(text)\n",
    "    predictions = {}\n",
    "    for i in range(6):\n",
    "        predictions[format(categories[i])] = models[i].predict([text])[0]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict_categories(models, categories, \"\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(categories)):\n",
    "    pickle.dump(models[i], open(\"sklearn_toxicity_models/model\" + str(i) + \".sav\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
